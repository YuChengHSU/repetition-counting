{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.4\n"
     ]
    }
   ],
   "source": [
    "## Library \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "from scipy import spatial\n",
    "import json\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import glob\n",
    "import cv2 as cv2\n",
    "import sklearn as sk\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "print(pd.__version__)\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "## Utils function from https://github.com/ChrisWu1997/2D-Motion-Retargeting\n",
    "from functional.visualization import motion2video, hex2rgb\n",
    "from functional.motion import preprocess_motion2d, postprocess_motion2d, openpose2motion\n",
    "from functional.utils import ensure_dir, pad_to_height\n",
    "from model import get_autoencoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from common import config\n",
    "from dataset import get_meanpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indicating the joint connection\n",
    "connect_dict=[[0,1],[1,2],[2,3],[3,4],[4,5],[3,6],[6,7],[7,8],[8,9],[3,10],[10,11],[11,12],[12,13],[0,14],[14,15],[15,16],[16,17],[0,18],[18,19],[19,20],[20,21]]\n",
    "pose_net_joint=[4,7,8,9,11,12,13,14,15,16,18,19,20]\n",
    "open_pose_joint=[4,2,11,12,13,7,8,9,0,18,19,20,14,15,16]\n",
    "## Sliding window size\n",
    "win_size=16\n",
    "## Output file directory\n",
    "output_dir='inc_deep_squat'\n",
    "## Task and participant name\n",
    "task_list=['m01','m02','m03','m04','m05','m06','m07','m08','m09','m10']\n",
    "s_number=['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10']\n",
    "e_number=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10']\n",
    "name_dict={'m01':'deep squat','m02':'hurdle step','m03':'inline lunge','m04':'side lunge','m05':'sit to stand','m06':'leg raise','m07':\t'shoulder abduction','m08':\t'shoulder extension','m09':\t'shoulder internal-external rotation','m10':'shoulder scaption'}\n",
    "\n",
    "## Mirroring the motion if the participants are doing in the minority side\n",
    "LR_table=pd.read_csv('LR_table.csv',index_col=0)\n",
    "## Kinect position/angle data location\n",
    "kin_position_path='Segmented Movements/Segmented Movements/Kinect/Positions/'\n",
    "kin_angle_path='Segmented Movements/Segmented Movements/Kinect/Angles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3D_corrd_in_view(kinect_postion,kinect_angle):\n",
    "    kinect_postion=np.array(kinect_postion)\n",
    "    kinect_angle=np.array(kinect_angle)\n",
    "    kinect_postion=kinect_postion.reshape(22,3)\n",
    "    kinect_angle=kinect_angle.reshape(22,3)\n",
    "    for j_p, j_c in connect_dict:\n",
    "        rot_mat=scipy.spatial.transform.Rotation.from_euler('yxz',kinect_angle[j_p,:3]*np.pi/180).as_matrix()\n",
    "        kinect_postion[j_c,:]=kinect_postion[j_p,:]+np.matmul(rot_mat,kinect_postion[j_c,:])   \n",
    "    return kinect_postion\n",
    "\n",
    "def plot_skelton(dat,**kwargs):\n",
    "    plt.annotate('Waist',(dat[0,0],-dat[0,1]))\n",
    "    plt.annotate('head',(dat[4,0],-dat[4,1]))\n",
    "    plt.annotate('left hand',(dat[9,0],-dat[9,1]))\n",
    "    plt.annotate('Right hand',(dat[13,0],-dat[13,1]))\n",
    "    plt.annotate('Left leg',(dat[17,0],-dat[17,1]))\n",
    "    for j1,j2 in connect_dict:\n",
    "        plt.plot(dat[[j1,j2],0],-dat[[j1,j2],1],**kwargs)\n",
    "        \n",
    "def LR_mirror(kinect_postion,percentage,LR_table,motion): \n",
    "    for i in range(len(percentage)):\n",
    "        if LR_table[motion][percentage.tester[i]]:\n",
    "            kinect_postion[i,:,0]=-kinect_postion[i,:,0]\n",
    "    return kinect_postion\n",
    "\n",
    "def poseNet2openPose(kinect_postion):\n",
    "    chest=np.mean(kinect_postion[:,[1,4],:],axis=1).reshape(-1,1,kinect_postion.shape[2])\n",
    "    base=np.mean(kinect_postion[:,[7,10],:],axis=1).reshape(-1,1,kinect_postion.shape[2])\n",
    "    new_openpose=kinect_postion[:,[0],:]\n",
    "\n",
    "    new_openpose=np.concatenate((new_openpose,chest),axis=1)\n",
    "    new_openpose=np.concatenate((new_openpose,kinect_postion[:,[4,5,6],:]),axis=1)\n",
    "    new_openpose=np.concatenate((new_openpose,kinect_postion[:,[1,2,3],:]),axis=1)\n",
    "    new_openpose=np.concatenate((new_openpose,base),axis=1)\n",
    "    new_openpose=np.concatenate((new_openpose,kinect_postion[:,[10,11,12],:]),axis=1)\n",
    "    new_openpose=np.concatenate((new_openpose,kinect_postion[:,[7,8,9],:]),axis=1)\n",
    "    return new_openpose\n",
    "\n",
    "def getPoseNet(motion):\n",
    "    return(motion[:,pose_net_joint,:])\n",
    "def getOpenPose(motion):\n",
    "    return(motion[:,open_pose_joint,:])\n",
    "def to_2D(motion):\n",
    "    return(motion[:,:,:2])\n",
    "\n",
    "def tl_preprocess_encode(motion,encoder,scale=1.2):\n",
    "    \n",
    "    motion=to_2D(motion)\n",
    "\n",
    "    if motion.shape[1] == 22:\n",
    "        motion=getOpenPose(motion)\n",
    "    elif motion.shape[1] == 13:\n",
    "        motion=poseNet2openPose(motion)\n",
    "    elif motion.shape[1] == 15:\n",
    "        motion=motion\n",
    "    else:\n",
    "        \"Not supported skeleton\"\n",
    "    for i in range(len(motion) - 1, 0, -1):\n",
    "        motion[i - 1][np.where(motion[i - 1] == 0)] = motion[i][np.where(motion[i - 1] == 0)]\n",
    "\n",
    "    motion = np.stack(motion, axis=2)\n",
    "    motion = gaussian_filter1d(motion, sigma=2, axis=-1)\n",
    "    motion = motion * scale\n",
    "    \n",
    "    motion=preprocess_motion2d(motion, mean_pose, std_pose)\n",
    "    motion=motion.to(config.device)\n",
    "    return encoder(motion).cpu().detach().numpy().squeeze()\n",
    "\n",
    "def get_sliding_wins(motion,percentage,win_size,steps=1):\n",
    "    percentage.reset_index(inplace=True,drop=True)\n",
    "    sliding_win=[]\n",
    "    sliding_percent=[]\n",
    "\n",
    "    for i in percentage.tester.unique():\n",
    "        start_id=np.where(percentage.tester == i)[0][0]\n",
    "        end_id=np.where(percentage.tester == i)[0][-1]+1\n",
    "        counter=0\n",
    "        for window in percentage.percentage.iloc[start_id:end_id].rolling(window=win_size,min_periods=win_size):\n",
    "            if len(window)<win_size:\n",
    "                continue\n",
    "            else:\n",
    "                if counter%steps == 0:\n",
    "                    sliding_win.append(motion[window.index,:,:])\n",
    "                    sliding_percent.append(window.iloc[-1])\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    counter+=1\n",
    "    sliding_percent=np.array(sliding_percent)\n",
    "    return(sliding_win,sliding_percent)\n",
    "\n",
    "def label_encoding(data, segment,win_size,encoder,steps=1):\n",
    "    sliding_win, sliding_percent=get_sliding_wins(data, segment,win_size,steps=steps)\n",
    "    encode_test=[]\n",
    "    for i in range(len(sliding_win)):\n",
    "        data=tl_preprocess_encode(sliding_win[i],encoder=encoder)\n",
    "        encode_test.append(data)\n",
    "    encode_test=np.array(encode_test)\n",
    "    # sim_matrix=sk.metrics.pairwise.cosine_similarity(encode_test.reshape((-1,96*3)))\n",
    "    return((encode_test, sliding_percent) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args= argparse.Namespace(name='skeleton',model_path='model/pretrained_skeleton.pth',v1='inc_deep_squat',o='inc_deep_squat',gpu_ids=0,w1=720,h1=720,transparency=False,save_frame=1,\n",
    " fps=25,color1='#a50b69#b73b87#db9dc3',max_len=480,max_frame=480)\n",
    "config.initialize(args)\n",
    "mean_pose, std_pose = get_meanpose(config)\n",
    "net = get_autoencoder(config)\n",
    "net.load_state_dict(torch.load(args.model_path))\n",
    "net.to(config.device)\n",
    "net.eval()\n",
    "encoder=nn.Sequential(*list(net.mot_encoder.children())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion: m10\n",
      "Testing on subject: s01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-692e58f34b38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mencode_X_train_camera\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_matrix_X_train_camera\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mlabel_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwin_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0e81442bdd06>\u001b[0m in \u001b[0;36mlabel_encoding\u001b[1;34m(data, segment, win_size, encoder, steps)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mencode_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msliding_win\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtl_preprocess_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msliding_win\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mencode_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mencode_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0e81442bdd06>\u001b[0m in \u001b[0;36mtl_preprocess_encode\u001b[1;34m(motion, encoder, scale)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mmotion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mmotion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgaussian_filter1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mmotion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmotion\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sdscphd\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[0msl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for task in task_list:\n",
    "    if task != 'm10':\n",
    "        continue\n",
    "    m_data=np.load('data/'+task+'.npy')\n",
    "    percentage=pd.read_csv('data/'+task+'.csv')\n",
    "    m_data=LR_mirror(m_data,percentage,LR_table,task)\n",
    "    m_data=getPoseNet(m_data)\n",
    "    for subj in s_number:\n",
    "        print(\"Motion: %s\"%task)\n",
    "        print('Testing on subject: %s'%subj)\n",
    "        X_train, y_train = np.array(m_data)[np.where(percentage['tester']!=subj),:,:][0],percentage.iloc[np.where(percentage['tester']!=subj)[0],:]\n",
    "        X_test, y_test = np.array(m_data)[np.where(percentage['tester']==subj),:,:][0],percentage.iloc[np.where(percentage['tester']==subj)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        encode_X_train_camera, sim_matrix_X_train_camera =  label_encoding(X_train, y_train,win_size,encoder,steps=1)\n",
    "\n",
    "\n",
    "        print(\"Finish encoding\")\n",
    "\n",
    "        X_train, y_train =sk.utils.shuffle(encode_X_train_camera,sim_matrix_X_train_camera)\n",
    "\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.Input(shape=(128,2,)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        # model.add(keras.layers.LSTM(32))\n",
    "\n",
    "        model.add(keras.layers.Conv1D(32, 3, activation='relu'))\n",
    "        model.add(keras.layers.MaxPooling1D(2))\n",
    "        model.add(keras.layers.Conv1D(64, 3, activation='relu'))\n",
    "        model.add(keras.layers.MaxPooling1D(2))\n",
    "        model.add(keras.layers.Flatten())\n",
    "\n",
    "        # model.add(keras.layers.Dense(64))\n",
    "        model.add(keras.layers.Dense(16))\n",
    "        model.add(keras.layers.Dense(8))\n",
    "        model.add(keras.layers.Dense(4))\n",
    "        model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "        model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=['MeanSquaredError']\n",
    "        )\n",
    "        print(model.summary())\n",
    "        model.fit(x=X_train.astype('float32'),y=y_train.astype('float32'), epochs=1000,validation_split=0.2,batch_size=50)\n",
    "        model.save('model/0516_full_full_Motion_sim_poseNet_'+task+\"_testing on_\"+subj+'.h5')\n",
    "\n",
    "        X_testcamera=np.array([camera_projet(x,np.array([0,0,0.0]),np.array([0,0.0,0])) for x in X_test])\n",
    "        encode_X_test_camera, sim_matrix_X_test_camera =  label_encoding(X_testcamera, y_test,win_size,encoder)\n",
    "\n",
    "        predict=model.predict(encode_X_test_camera)\n",
    "\n",
    "        perfor[task][subj]=sk.metrics.mean_squared_error(sim_matrix_X_test_camera,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion: m01\n",
      "Testing on subject: s01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdscphd\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'name_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dd00d86af583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mperfor_t_diff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mframe_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Motion: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" testing on \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperfor_lb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperfor_lb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mperfor_r\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Lower bound'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperfor_lb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxytext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperfor_lb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marrowprops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrowstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnectionstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"arc3,rad=0.2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name_dict' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "win_size=16\n",
    "args= argparse.Namespace(name='skeleton',model_path='model/pretrained_skeleton.pth',v1='inc_deep_squat',o='inc_deep_squat',gpu_ids=0,w1=720,h1=720,transparency=False,save_frame=1,\n",
    "                         fps=25,color1='#a50b69#b73b87#db9dc3',max_len=480,max_frame=480)\n",
    "config.initialize(args)\n",
    "mean_pose, std_pose = get_meanpose(config)\n",
    "net = get_autoencoder(config)\n",
    "net.load_state_dict(torch.load(args.model_path))\n",
    "net.to(config.device)\n",
    "net.eval()\n",
    "encoder=nn.Sequential(*list(net.mot_encoder.children())[0])\n",
    "perfor=pd.DataFrame(columns=task_list,index=s_number)\n",
    "perfor_lb=pd.DataFrame(columns=task_list,index=s_number)\n",
    "perfor_r=pd.DataFrame(columns=task_list,index=s_number)\n",
    "perfor_t=pd.DataFrame(columns=task_list,index=s_number)\n",
    "perfor_mse=pd.DataFrame(columns=task_list,index=s_number)\n",
    "perfor_t_diff=pd.DataFrame(columns=task_list,index=s_number)\n",
    "lb=np.arange(0.1,1,0.05)\n",
    "bound=np.arange(0.2,1,0.05)\n",
    "for task in task_list:\n",
    "    m_data=np.load('data/'+task+'.npy')\n",
    "    percentage=pd.read_csv('data/'+task+'.csv')\n",
    "    m_data=LR_mirror(m_data,percentage,LR_table,task)\n",
    "    m_data=getOpenPose(m_data)\n",
    "    space=pd.DataFrame(index=lb,columns=bound)\n",
    "    tot_time=0\n",
    "    tot_frame=0\n",
    "    for subj in s_number:\n",
    "        print(\"Motion: %s\"%task)\n",
    "        print('Testing on subject: %s'%subj)\n",
    "        X_train, y_train = np.array(m_data)[np.where(percentage['tester']!=subj),:,:][0],percentage.iloc[np.where(percentage['tester']!=subj)[0],:]\n",
    "        X_test, y_test = np.array(m_data)[np.where(percentage['tester']==subj),:,:][0],percentage.iloc[np.where(percentage['tester']==subj)]\n",
    "\n",
    "        model=tf.keras.models.load_model('0516_model/0516_full_full_Motion_sim_poseNet_'+task+\"_testing on_\"+subj+'.h5')\n",
    "\n",
    "        start=time.time()\n",
    "        encode_X_test_camera, sim_matrix_X_test_camera =  label_encoding(X_test, y_test,win_size,encoder)\n",
    "        predict=model.predict(encode_X_test_camera)\n",
    "        end=time.time()\n",
    "        perfor_t[task][subj]=(end-start)/encode_X_test_camera.shape[0]\n",
    "        perfor_mse[task][subj]=sk.metrics.mean_squared_error(sim_matrix_X_test_camera,predict)\n",
    "        frame_time= np.where(sim_matrix_X_test_camera==1)[0]\n",
    "        frame_time=np.concatenate((frame_time,[len(sim_matrix_X_test_camera)]))\n",
    "        previous_best=[]\n",
    "        current_frame=[]\n",
    "        for l in lb:\n",
    "            for ran in bound:\n",
    "                if l+ran >1:\n",
    "                    continue\n",
    "                count=0\n",
    "                low_swtich=True\n",
    "                high_switch=False\n",
    "                current_frame=[]\n",
    "                for i in range(len(predict)):\n",
    "                    if predict[i]<l and low_swtich:\n",
    "                        high_switch=True\n",
    "                        low_swtich=False\n",
    "                    if predict[i]>l+ran and high_switch:\n",
    "                        count=count+1\n",
    "                        high_switch=False\n",
    "                        low_swtich=True\n",
    "                        current_frame.append(i)\n",
    "                if space[ran][l] is np.nan:\n",
    "                    space[ran][l]=np.abs(count-10)/10\n",
    "#                     if count not in [9, 10, 11]:\n",
    "#                         space[ran][l]=1\n",
    "#                     else:\n",
    "#                         space[ran][l]=0\n",
    "                else:\n",
    "                    space[ran][l]=np.abs(count-10)/10+space[ran][l]\n",
    "\n",
    "#                     if count not in [9, 10, 11]:\n",
    "#                         space[ran][l]=1+ space[ran][l]\n",
    "#                     else:\n",
    "#                         space[ran][l]=0+ space[ran][l]\n",
    "                if perfor[task][subj] is np.nan or (np.abs(count-10)<np.abs(perfor[task][subj]-10)) or (np.abs(count-10)==np.abs(perfor[task][subj]-10) and ran> perfor_r[task][subj]):       \n",
    "                    perfor[task][subj]= count\n",
    "                    perfor_lb[task][subj]=l\n",
    "                    perfor_r[task][subj]=ran\n",
    "                if count ==10:\n",
    "                    if previous_best == []:\n",
    "                         previous_best=np.array(current_frame)\n",
    "                    elif sum(np.abs(np.array(current_frame)+12-frame_time)) < sum(np.abs(np.array(previous_best)-frame_time)):\n",
    "                        previous_best=np.array(current_frame)+12\n",
    "        if len(previous_best) ==10:\n",
    "            perfor_t_diff[task][subj]=np.mean(np.abs(np.array(previous_best)-frame_time)) \n",
    "        fig, ax= plt.subplots()\n",
    "        ax.set_title(\"Motion: \"+name_dict[task]+\" testing on \"+subj)\n",
    "        ax.hlines([perfor_lb[task][subj],perfor_lb[task][subj]+perfor_r[task][subj]],0,len(predict))\n",
    "        ax.annotate('Lower bound',(10,perfor_lb[task][subj]),xytext=(30,perfor_lb[task][subj]+0.1),arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.2\"))\n",
    "        ax.annotate('Upper bound',(10,perfor_lb[task][subj]+perfor_r[task][subj]),xytext=(30,perfor_lb[task][subj]+perfor_r[task][subj]-0.1),arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.2\"))\n",
    "        ax.plot(predict)\n",
    "        ax.plot(sim_matrix_X_test_camera)\n",
    "        ax.set_xlabel(\"Frame\")\n",
    "        ax.set_ylabel(\"Progress\")\n",
    "        ax.legend(['Predicted value','Groud Truth'])\n",
    "    fig, ax= plt.subplots(subplot_kw={\"projection\": \"3d\"},figsize=(7,6))\n",
    "    ax.set_title(name_dict[task]+\" MAE\")\n",
    "    X, Y = np.meshgrid(bound,lb)\n",
    "    surf = ax.plot_surface( Y,X,  np.array(space.fillna(10)/10), cmap=cm.coolwarm,\n",
    "                   linewidth=0, antialiased=False)\n",
    "    surf.set_clim(0,1) \n",
    "    ax.set_xlabel('Lower bound')\n",
    "    ax.set_ylabel('Range')\n",
    "    ax.set_zlabel('MAE')\n",
    "    fig.colorbar(surf,ax=ax)\n",
    "\n",
    "    fig.savefig('fig/motion_MAE_1_'+task)\n",
    "#         ax.plot(sliding_percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select task and subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0516_full_full_Motion_sim_poseNet_m01_testing on_s01.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion='m01'\n",
    "subject='s01'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: 0516_model/0516_full_full_Motion_sim_poseNet_m01_testing on_m01.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b387307cfa15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tester'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tester'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'0516_model/0516_full_full_Motion_sim_poseNet_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_testing on_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmotion\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\sdscphd\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sdscphd\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    112\u001b[0m                   (export_dir,\n\u001b[0;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: 0516_model/0516_full_full_Motion_sim_poseNet_m01_testing on_m01.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "m_data=np.load('data/'+motion+'.npy')\n",
    "percentage=pd.read_csv('data/'+motion+'.csv')\n",
    "X_test, y_test = np.array(m_data)[np.where(percentage['tester']==subject),:,:][0],percentage.iloc[np.where(percentage['tester']==subject)]\n",
    "model=tf.keras.models.load_model('0516_model/0516_full_full_Motion_sim_poseNet_'+motion+\"_testing on_\"+motion+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0516_full_full_Motion_sim_poseNet_m01_testing on_s01.h5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
